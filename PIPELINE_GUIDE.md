# DE-LU Power Pipeline: User Guide

This guide describes the pipeline used for the Germany–Luxembourg (DE-LU) day-ahead power market: data sources, features, models, curve translation, and how outputs are used. The pipeline is run from the repo root with `python run_all.py`. Code lives in the `src` package; run tasks as `python -m src.<module>` from the repo root (do not run scripts directly).

---

## 1. Data and quality checks

An hourly dataset is built from the public [Energy Charts API](https://api.energy-charts.info/). Three endpoints are used: **day-ahead prices** (`/price`, `bzn=DE-LU`), **wind, solar and load** (`/public_power`, `country=de`), and **cross-border physical flows** (`/cbpf`, `country=de`). Requests are made in monthly chunks. Timestamps are stored in UTC and converted to Europe/Berlin; DST is handled (23h on spring-forward day, 25h on fall-back day). After merging and resampling to one row per hour, the dataset contains **price_eur_mwh** (the forecast target), **wind_onshore_MW**, **wind_offshore_MW**, **solar_MW**, **load_MW**, and **net_import_GW**.

**Baseline QA** is run first and written to `reports/qa_report.json`: missingness per column, duplicate timestamps, coverage (expected vs actual hours), outliers (±3×IQR per numeric column), DST hour counts, and negative prices. The raw merged series is then cleaned (duplicate index dropped, null target dropped) and saved as `data/de_hourly_baseline.parquet`. **No LLM-applied QC:** The pipeline does not apply any additional validation rules from the LLM. The baseline-cleaned data is copied to `data/de_hourly_dataset.parquet` for downstream tasks (forecast, curve translation). The LLM is used only to **write a short QA narrative** from computed metrics: coverage and DST status, price percent negative, net-import flatline (longest run of identical values), and price volatility with **explicit date ranges** (last 7 days vs prior 7 days, with `volatility_window_current` and `volatility_window_prior` in Berlin time so “this week” is unambiguous). All numbers in the narrative come from code; the LLM does not invent figures. The narrative is written to `reports/qa_narrative.txt`; the metrics and log (model, ts_utc, latency_s, prompt, output, error) are in `reports/qa_llm_report.json` and `reports/qa_narrative_log.json`. Figure 1 in `figures/` summarises monthly coverage and the baseline QA table.

---

## 2. Features and models

The target is next-day hourly DA price. All inputs are restricted to information known at or before the day-ahead auction (no look-ahead). **Calendar features** (hour, day_of_week, month, is_weekend) are used for the prediction hour and day. **Fundamental features** are lagged by 24 hours: wind_onshore_MW_lag24, wind_offshore_MW_lag24, solar_MW_lag24, load_MW_lag24, net_import_GW_lag24, plus wind_total_MW and ren_penetration derived from these lagged values. Thus, when predicting day D, only data from D−1 and earlier is used for fundamentals. **Price history** includes price_lag_24h, price_lag_168h, price_rmean_7d, and price_rstd_7d (all lagged so that no same-day price is used).

**Forecast input:** The forecast step loads `data/de_hourly_dataset.parquet` if present; if not (e.g. only Task 1 was run), it falls back to `data/de_hourly_baseline.parquet` so Task 2 can be run after Task 1 without running the narrative step.

Four models are run in walk-forward validation. **(1) Seasonal Naive** uses only price_lag_168h (same hour last week); no fitting is performed. **(2) Ridge Regression** is a linear model on all features (calendar, lagged fundamentals, price lags and rolling), with standardisation. **(3) Random Forest** is an ensemble of trees on the same full feature set. **(4) LightGBM** is gradient boosting on all features, with early stopping on a **true holdout** (last 720 hours of the training window are excluded from fitting and used only for early stopping); its forecasts are used for curve translation, signals, and the submission file. For each test day, training is performed on all data before that day; models are retrained every 7 days. At least 720 hours of valid training data (after dropping rows with missing features) are required before predictions are made. **Test hours** with NaN in any feature or the target are dropped before prediction; the number of dropped test hours is logged if non-zero. Metrics reported for each model are MAE, RMSE, bias, and **tail MAE** (MAE on the bottom 5% and top 5% of actual price hours). Spikes are the hardest regime; tail error is quantified explicitly. All metrics are written to `reports/forecast_results.json` and shown in Figure 3 (four panels: MAE, RMSE, bias, tail MAE). Predictions are saved to `data/forecast_predictions.csv`.

**Forward expected averages** are derived from the forecast distribution and written to `reports/forecast_results.json` (and echoed in the curve report). **(1) Next day:** A one-day-ahead forward forecast is produced by training on all available data and predicting the next calendar day (24 hours) using lagged features; the mean of those 24 hourly LightGBM predictions is **next_day_expected_avg** (EUR/MWh). **(2) Next week:** The mean of the LightGBM hourly forecasts over the **last 7 days** of the test window is **next_week_expected_avg** (with date range, e.g. “next_week_dates”). **(3) Next month:** The mean of the LightGBM hourly forecasts over the **last calendar month** in the test window is **next_month_expected_avg** (with “next_month_label”, e.g. 2025-12). These three numbers are the expected average price for the next day, next week, and next month implied by the model’s forecast distribution.

---

## 3. Curve translation and trading use

Hourly LightGBM forecasts are turned into delivery-period views, **next-week / next-month expected average** from the forecast distribution, and a directional signal. **Forward expected averages** (next day, next week, next month) are read from `reports/forecast_results.json` and included in `reports/curve_trading_sheet.json` and the curve trading text file so the desk has a single place for “expected average” over the next day, next 7 days, and next calendar month. **Delivery-period means** (daily and monthly) are computed as the average forecast and average actual over the relevant hours (base); these are used as a reference level for prompt month or day. **Uncertainty bands (P10–P90)** are built from past forecast errors (e.g. last 30 days): the 10th and 90th percentiles of the error distribution are added to the point forecast for each day’s average, giving a likely range; position size can be scaled inversely to band width. **Risk premium** is supported by the same delivery means: e.g. forecast average minus trailing realised average indicates whether the model sees the market as rich or cheap relative to recent history.

The **directional signal (BUY / SELL / NEUTRAL)** is based on yesterday’s realised daily average as anchor. For today, the LightGBM (and other models’) average forecast is compared to the anchor; a score combines the forecast–anchor gap (e.g. z-score) and agreement across the four models (naive, Ridge, Random Forest, LightGBM). From this score, BUY is assigned when the view is above the anchor, SELL when below, and NEUTRAL otherwise. In the backtest, a BUY is correct if actual daily average exceeds the anchor; a SELL is correct if actual is below the anchor. Hit rate and P&L are written to `reports/curve_trading_sheet.json` and the curve trading text file.

**Desk use:** Delivery-period means are used as a reference for prompt month or quarter; shape trades can be expressed when the forecast profile differs from the curve; P10–P90 bands are used for position sizing; the BUY/SELL/NEUTRAL signal is used as a daily bias check and should be combined with liquidity, risk limits, and fundamental news. **Invalidation:** The signal should be ignored or downweighted when wind/solar or demand is expected to deviate strongly from assumptions, when unplanned outages occur, when model performance degrades (e.g. rolling MAE rises), or when liquidity is poor. The output is treated as one input among others, not as a single source of truth.

---

## 4. AI commentary, run order and outputs

**AI commentary (LangChain, Ollama local only):** The LLM is used in two places. **(1) QA narrative (after baseline):** The baseline-cleaned data is not filtered by the LLM. The pipeline copies it to `data/de_hourly_dataset.parquet` and computes narrative metrics in code (coverage, DST, price % negative, net-import flatline, and volatility with explicit date-range windows: `volatility_window_current`, `volatility_window_prior`, `volatility_std_current`, `volatility_std_prior`, `volatility_pct_change`). The LLM is given only these numbers and asked to write a short QA report (e.g. “All timestamps present, DST ok.”; “Price has X% negative hours.”; “In YYYY-MM-DD to YYYY-MM-DD, price volatility (std) was A vs B in [prior window] (C%).”). No figures are invented. The run is logged to `reports/qa_narrative_log.json` with **model**, **ts_utc**, **latency_s**, prompt (truncated), output, and error. **(2) Drivers commentary:** A short daily note is generated from pipeline metrics only (model performance, signal backtest, recent signals); the LLM is instructed to cite source files (e.g. `reports/forecast_results.json`) so the note links back to the underlying tables. The note is written to `reports/ai_commentary.txt`. If the Ollama call fails (e.g. Ollama not running), a fallback template is used. Default model is `qwen2.5:7b`; set `LLM_MODEL` or `LLM_BASE_URL` in the environment to override (see `src/config.py`).

**Run order:** (1) Ingest and baseline QA (`python -m src.main`); (2) Copy baseline to dataset and write QA narrative from computed metrics (`run_llm_qa_and_save_cleaned`, invoked from `run_all.py`); (3) Forecast (`python -m src.forecast`); (4) Curve translation (`python -m src.curve_translation`); (5) AI commentary / drivers note (`python -m src.ai_commentary`); (6) Submission and figures (`python -m src.generate_submission`). The runner at the repo root is `run_all.py`; all pipeline source code is under `src/`.

**Outputs:** `data/` — `de_hourly_baseline.parquet`, `de_hourly_dataset.parquet` (copy of baseline), `forecast_predictions.csv`. `reports/` — `qa_report.json`, `qa_llm_report.json` (narrative metrics and dataset stats), `qa_narrative.txt`, `qa_narrative_log.json` (QA narrative: model, ts_utc, latency_s, prompt, output, error), `forecast_results.json` (includes **forward_expected_averages**: next_day_expected_avg, next_week_expected_avg, next_month_expected_avg with dates/labels), `curve_trading_sheet.json` (and .txt; includes **forward_expected_averages**), `ai_commentary.txt`, `ai_commentary_log.json` (drivers commentary: model, ts_utc, latency_s, prompt, output, error). `figures/` — Fig 1 (data quality), Fig 2 (forecast vs actual), Fig 3 (model comparison: MAE, RMSE, bias, tail MAE). **`submission.csv`** — out-of-sample predictions with columns **id** (hourly timestamp UTC) and **y_pred**; the first line is a comment stating the **test window** (e.g. `2025-10-01 to 2025-12-31`), defined in `src/config.py` as `TEST_START` and `TEST_END`. Dependencies are installed with `pip install -r requirements.txt`; the pipeline is executed with `python run_all.py` from the repo root.
